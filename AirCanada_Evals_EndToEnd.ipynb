{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076436cb",
   "metadata": {},
   "source": [
    "\n",
    "# AirCanada Hallucination Evals — End‑to‑End (LLM‑first)\n",
    "\n",
    "This notebook runs the entire pipeline Rune asked for:\n",
    "\n",
    "1) **Generate scenarios** (LLM‑synthesized; template fallback)  \n",
    "2) **Run the multi‑turn evals** against a toy support bot (with groundedness filter & escalation)  \n",
    "3) **Judge** with two independent rules and **escalate** on disagreement/low‑confidence  \n",
    "4) **Aggregate & visualize** coverage  \n",
    "5) Inspect **flagged examples** and **re‑run** a scenario to show the multi‑turn transcript\n",
    "\n",
    "> **Prereqs:** Run this at your repo root (`aiuc_aircanada_eval/`).  \n",
    "> **Providers:** Set `PROVIDER=openai` + `OPENAI_API_KEY`, or `PROVIDER=anthropic` + `ANTHROPIC_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b8f18",
   "metadata": {},
   "source": [
    "## 0) Setup (first run only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not installed these yet, uncomment and run:\n",
    "# !pip install openai anthropic matplotlib pandas\n",
    "\n",
    "import os, sys, json, pathlib, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path.cwd()\n",
    "print(\"Working dir:\", BASE)\n",
    "\n",
    "# Show provider hint (set these in your environment or here before running step 1)\n",
    "print(\"PROVIDER =\", os.environ.get(\"PROVIDER\", \"(not set; default 'openai')\"))\n",
    "print(\"OPENAI_API_KEY set? \", bool(os.environ.get(\"OPENAI_API_KEY\")))\n",
    "print(\"ANTHROPIC_API_KEY set? \", bool(os.environ.get(\"ANTHROPIC_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c86f0",
   "metadata": {},
   "source": [
    "## 1) Generate scenarios (LLM‑first; template fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefer LLM mode\n",
    "cmd_llm = [sys.executable, \"src/generate_scenarios.py\", \"--per-cell\", \"2\", \"--mode\", \"llm\"]\n",
    "print(\"Running:\", \" \".join(cmd_llm))\n",
    "ret = subprocess.run(cmd_llm)\n",
    "scen_path = Path(\"evals/scenarios.jsonl\")\n",
    "\n",
    "# Fallback to template if generation failed or file missing\n",
    "if ret.returncode != 0 or not scen_path.exists():\n",
    "    print(\"LLM generation failed or scenarios missing; falling back to template.\")\n",
    "    cmd_tpl = [sys.executable, \"src/generate_scenarios.py\", \"--per-cell\", \"2\", \"--mode\", \"template\"]\n",
    "    subprocess.check_call(cmd_tpl)\n",
    "\n",
    "print(\"Scenarios written to:\", scen_path)\n",
    "print(\"\\nPreview (first 3 lines):\")\n",
    "print(\"\\n\".join(scen_path.read_text().splitlines()[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2111c",
   "metadata": {},
   "source": [
    "## 2) Run the eval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043500f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex, json\n",
    "subprocess.check_call([sys.executable, \"src/run.py\"])\n",
    "\n",
    "agg_path = Path(\"evals/aggregate.json\")\n",
    "print(\"Aggregate:\")\n",
    "print(agg_path.read_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd0f22",
   "metadata": {},
   "source": [
    "## 3) Visualize coverage (heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "from IPython.display import Image, display\n",
    "\n",
    "subprocess.check_call([sys.executable, \"src/viz.py\"])\n",
    "img_path = Path(\"images/coverage_heatmap.png\")\n",
    "display(Image(filename=str(img_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea7e59",
   "metadata": {},
   "source": [
    "## 4) Inspect flagged examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pandas as pd\n",
    "df = pd.read_csv(\"evals/results.csv\")\n",
    "flags = df[df[\"label\"] != \"safe\"]\n",
    "print(\"Total flags:\", len(flags))\n",
    "flags.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b4937",
   "metadata": {},
   "source": [
    "## 5) Re‑run a scenario to view the multi‑turn transcript + verdicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e59095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sut import SupportBot\n",
    "from judge import judge_dialogue, judge_dialogue_alt\n",
    "import json\n",
    "\n",
    "policy = Path(\"policy/policy.txt\").read_text()\n",
    "bot = SupportBot(policy)\n",
    "\n",
    "# Load the first scenario for demo\n",
    "scen = json.loads(open(\"evals/scenarios.jsonl\").read().splitlines()[0])\n",
    "history = []\n",
    "for turn in scen['dialogue']:\n",
    "    if turn['role'] == 'user':\n",
    "        # append user turn\n",
    "        history.append(turn)\n",
    "        # bot replies\n",
    "        reply = bot.answer(history, {\"attack\": scen[\"attack\"], \"use_case\": scen[\"use_case\"]})\n",
    "        history.append({\"role\":\"assistant\",\"content\": reply})\n",
    "\n",
    "print(\"ATTACK:\", scen['attack'], \"| USE_CASE:\", scen['use_case'])\n",
    "print(\"\\n--- Transcript ---\")\n",
    "for m in history:\n",
    "    role = m['role'].upper()\n",
    "    print(f\"{role}: {m['content']}\")\n",
    "\n",
    "print(\"\\n--- Judges ---\")\n",
    "print(\"Judge A:\", judge_dialogue(history, policy))\n",
    "print(\"Judge B:\", judge_dialogue_alt(history, policy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c835a",
   "metadata": {},
   "source": [
    "## 6) Groundedness KPI (citation rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate: count how many example_bot_utterance rows include a [§N] citation\n",
    "df = pd.read_csv(\"evals/results.csv\")\n",
    "has_cite = df[\"example_bot_utterance\"].astype(str).str.contains(\"\\[§\")\n",
    "rate = has_cite.mean()\n",
    "print(f\"Citation presence in final utterance: {rate:.2%}  (target: ~100% after guardrail)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f995cc",
   "metadata": {},
   "source": [
    "## 7) Generate buyer-facing summary (Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "from report_gen import generate_buyer_report\n",
    "print(\"Report path:\", generate_buyer_report(Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 8) Residual risk bands (use case / tactic / combo)\n",
    "import pandas as pd\n",
    "from risk import compute_risk_summaries\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"evals/results.csv\")\n",
    "risk = compute_risk_summaries(df)\n",
    "print(\"By use case:\")\n",
    "pd.DataFrame(risk[\"by_use_case\"]).sort_values(\"risk_index\", ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
